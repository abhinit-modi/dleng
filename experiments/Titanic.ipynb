{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\n# System\nimport math\nimport os\nimport sys\n\n# Data\nimport numpy as np\nimport pandas as pd\n\n# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import clear_output\n\n# Util\nfrom six.moves import urllib\nfrom sklearn.model_selection import train_test_split\n\n# MachineLearning\ntry:\n  # %tensorflow_version only exists in Colab.\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow.compat.v2.feature_column as fc\n\nimport tensorflow as tf\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.compat.v1.enable_eager_execution()\n\npd.options.display.float_format = '{:.2f}'.format\npd.options.display.max_rows = 15","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data = train_data.reindex(np.random.permutation(train_data.index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = [\"Pclass\", \"Sex\", \"Embarked\"]\nnumeric_columns = [\"PassengerId\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\nembedding_columns = [\"Name\", \"Ticket\", \"Cabin\"]\nlabel = \"Survived\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling nas usually requires more than just doing 0 or median. Try to divide the column into multiple groups (based on correlations of categorical features) and within each group find median and assign\n\nfor dataset in [train_data, test_data]:\n    for i, sex in enumerate(['male', 'female']):\n        for j, pclass in enumerate([1, 2, 3]):\n            guess_df = dataset[(dataset['Sex'] == sex) & (dataset['Pclass'] == pclass)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == sex) & (dataset.Pclass == pclass), 'Age'] = int( age_guess/0.5 + 0.5 ) * 0.5\n\n    dataset['Age'] = dataset['Age'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature_name in numeric_columns:\n    train_data[feature_name] = pd.to_numeric(train_data[feature_name], errors='coerce')\n\nmean_age = np.mean(train_data[\"Age\"])\n# print(mean_age)\n# dftrain.head(20)\n# train_data[\"Age\"].fillna(0, inplace=True)\n# train_data[\"Cabin\"].fillna(\"\", inplace=True)\n# train_data[\"Embarked\"].fillna(\"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for feature_name in numeric_columns:\n#     test_data[feature_name] = pd.to_numeric(train_data[test_data], errors='coerce')\n\nmean_age = np.mean(test_data[\"Age\"])\n# print(mean_age)\n# dftrain.head(20)\n\n# test_data[\"Age\"].fillna(0, inplace=True)\ntest_data[\"Cabin\"].fillna(\"\", inplace=True)\ntest_data[\"Embarked\"].fillna(\"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_data[numeric_columns + [label]].astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Single Feature Analysis\n# NumericalFeature vs Label\n# Binning\ng = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pal = {'male':\"green\", 'female':\"Pink\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.barplot(x = \"Sex\", \n                 y = \"Survived\", \n                 data=train_data, \n                 palette = pal,\n                 linewidth=5,\n                 order = ['female','male'],\n                 capsize = .05,\n\n                )\n\nplt.title(\"Survived/Non-Survived Passenger Gender Distribution\", fontsize = 25,loc = 'center', pad = 40)\nplt.ylabel(\"% of passenger survived\", fontsize = 15, )\nplt.xlabel(\"Sex\",fontsize = 15);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Single Feature Analysis\n# Kernel Density Plot\n# Numerical Feature\nfig = plt.figure(figsize=(15,8),)\n## I have included to different ways to code a plot below, choose the one that suites you. \nax=sns.kdeplot(train_data.Pclass[train_data.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(train_data.loc[(train_data['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived', \n              )\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Passenger Class\", fontsize = 15,labelpad =20)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(train_data.Pclass.unique()), labels);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = sns.FacetGrid(train_data, hue='Sex', size=5.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', alpha=.5, bins=20)\ngrid.add_legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Objective is to move the point in the point plots towards the ends of the Y axis i.e <0.25 and >0.75\n\ngrid = sns.FacetGrid(train_data, col='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_data, size=5,hue=\"Survived\", col =\"Sex\", margin_titles=True)\ng.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\ng.fig.suptitle(\"Survived by Sex, Fare and Age\", size = 25)\nplt.subplots_adjust(top=0.85)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(abs(train_data.corr()['Survived']).sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------------------------------------------------------**FEATURE COLUMN MANAGEMENT**-----------------------------------------------------------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL = \"Survived\"\nCATEGORICAL_COLUMNS = ['Sex', 'Pclass', 'SibSp', 'Parch']\nNUMERIC_COLUMNS = ['Age']\nLABEL = 'Survived'\n\ndftrain, dfeval = train_test_split(train_data, test_size=0.3)\n\nytrain = dftrain.pop(LABEL)\nyeval = dfeval.pop(LABEL)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain.head()\nytrain.head()\n# dftrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nans = lambda df: df[df.isnull().any(axis=1)]\nnans(dftrain[[\"Fare\"]])\n\ndftrain[\"Fare\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain['LogFare'] = np.log(dftrain['Fare']+1)\ndfeval['LogFare'] = np.log(dfeval['Fare']+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NumericColumn\nfor feature_name in NUMERIC_COLUMNS:\n    dftrain[feature_name].hist(bins=150)\n\n# dftrain['Fare'].describe()\n# CategoricalColumn\n# for feature_name in CATEGORICAL_COLUMNS:\n#     dftrain[feature_name].value_counts().plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_columns = []\ncategorical_feature_columns = []\ncrossed_feature_columns = []\nnumeric_feature_columns = []\n\nfor feature_name in CATEGORICAL_COLUMNS:\n    vocabulary = dftrain[feature_name].unique()\n    categorical_feature_columns.append(\n        feature_column.indicator_column(\n            tf.feature_column.categorical_column_with_vocabulary_list(\n                feature_name, \n                vocabulary\n            )\n        )\n    )\n\nepsilon = 0.1\n\n# for feature_name in NUMERIC_COLUMNS:\n#     numeric_feature_columns.append(\n#         tf.feature_column.numeric_column(\n#             feature_name, \n#             dtype=tf.float32,\n#             normalizer_fn=lambda val: (val - dftrain.mean()[feature_name]) / (epsilon + dftrain.std()[feature_name])\n#         )\n#     )\n    \nnumeric_feature_columns.append(tf.feature_column.numeric_column(\n            'LogFare', \n            dtype=tf.float32\n        ))\n\ncrossed_feature = feature_column.crossed_column([tf.feature_column.categorical_column_with_vocabulary_list(\n                'SibSp', \n                vocabulary\n            ), tf.feature_column.categorical_column_with_vocabulary_list(\n                'Parch', \n                vocabulary\n            )], hash_bucket_size=15)\ncrossed_feature = feature_column.indicator_column(crossed_feature)\ncrossed_feature_columns.append(crossed_feature)\n\nage_buckets = feature_column.bucketized_column(tf.feature_column.numeric_column(\n            feature_name, \n            dtype=tf.float32,\n            normalizer_fn=lambda val: (val - dftrain.mean()[feature_name]) / (epsilon + dftrain.std()[feature_name])\n        ), boundaries=[5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70])\n\nfeature_columns.append(age_buckets)\nfeature_columns.extend(categorical_feature_columns[:2]) \nfeature_columns.extend(crossed_feature_columns)\nfeature_columns.extend(numeric_feature_columns)\nprint(len(feature_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_layer = tf.keras.layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Config\nBATCH_SIZE = 10\nNUM_EPOCHS = 50\nNUM_TRAINING_STEPS = 10000\n\nprint(len(dftrain))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# InputFunctors\ndef df_to_dataset(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n    # Dataset needs to be re-assigned\n    if shuffle:\n        ds = ds.shuffle(1000)\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    return ds\n\ndef testdf_to_dataset(data_df, num_epochs=10, shuffle=True, batch_size=32):\n    ds = tf.data.Dataset.from_tensor_slices(dict(data_df))\n    # Dataset needs to be re-assigned\n    if shuffle:\n        ds = ds.shuffle(1000)\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    return ds\n\ntrain_ds = df_to_dataset(dftrain, ytrain, num_epochs=NUM_EPOCHS, shuffle=True, batch_size=BATCH_SIZE)\neval_ds = df_to_dataset(dfeval, yeval, num_epochs=1, shuffle=False, batch_size=BATCH_SIZE)\ntest_ds = testdf_to_dataset(test_data, num_epochs=1, shuffle=False, batch_size=BATCH_SIZE)\n\nds = df_to_dataset(dftrain, ytrain, batch_size=5)\nfor feature_batch, label_batch in ds.take(1):\n    print('Length of batch: ', len(label_batch))\n    print('Number of features: ', len(feature_batch))\n    print('Some feature keys:', list(feature_batch.keys()))\n    print('A batch of Labels:', label_batch.numpy())\n    print()\n\n# InspectInput of model\ndef demo(example_batch, feature_column):\n    feature_layer = layers.DenseFeatures(feature_column)\n    print(feature_layer(example_batch).numpy())\n\nexample_batch = next(iter(ds))[0]\ndemo(example_batch, feature_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n  feature_layer,\n  layers.Dense(6, activation='sigmoid'),\n#   layers.Dense(16, activation='sigmoid'),\n  layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(train_ds,\n          validation_data=eval_ds,\n          epochs=5)\n\n\n# loss, accuracy = model.evaluate(test_ds)\n# print(\"Accuracy\", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_input_fn(data_df, label_df, n_epochs=None, shuffle=True, batch_size=32):\n    def input_fn():\n        return df_to_dataset(data_df, label_df, n_epochs, shuffle, batch_size)\n    return input_fn\n\ntrain_input_fn = make_input_fn(dftrain, ytrain, n_epochs=NUM_EPOCHS, shuffle=True, batch_size=BATCH_SIZE)\neval_input_fn = make_input_fn(dfeval, yeval, n_epochs=1, shuffle=False, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_batches = 5\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\n                                          n_batches_per_layer=n_batches)\n\n# The model will stop training once the specified number of trees is built, not\n# based on the number of steps.\nest.train(train_input_fn, max_steps=100)\n\n# Eval.\nresult = est.evaluate(eval_input_fn)\nclear_output()\nprint(pd.Series(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [1 if x>=0.5 else 0 for x in model.predict(test_ds) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame()\noutput[\"PassengerId\"] = test_data[\"PassengerId\"]\noutput[\"Survived\"] = predictions\n\noutput.to_csv(\"/kaggle/working/predictions_3fcross_83.58.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}